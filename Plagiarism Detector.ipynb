{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71f43d45",
   "metadata": {},
   "source": [
    "Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m nltk.downloader 'punkt' \n",
    "! python -m nltk.downloader 'stopwrods'\n",
    "! python -m nltk.downloader 'wordnet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0bb8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_files_path = '/Users/shahriar/Downloads/arxiv dataset'\n",
    "arxiv_10000_path = f'{base_data_files_path}/arxiv_10000.json'\n",
    "arxiv_complete_path = f'{base_data_files_path}/arxiv-metadata-oai-snapshot.json'\n",
    "temp_dir = 'tmp/'\n",
    "test_articles_dir = 'test_articles/'\n",
    "articles_to_plagiarise_dir = 'articles_to_plagiarise/'\n",
    "test_metadata_path = f'test_metadata/metadata.json'\n",
    "exact_copies_count = 50\n",
    "exact_copies_range = (0, 50)\n",
    "near_copies_count = 50\n",
    "near_copies_range = (50, 100)\n",
    "substitutes_count = 300\n",
    "substitutes_range= (100, 400)\n",
    "paraphrases_count = 5\n",
    "paraphrases_range = (400, 410)\n",
    "base_test_article_id = '0706.0638'\n",
    "base_article_url = 'https://arxiv.org/pdf'\n",
    "\n",
    "searchable_fields = ['title', 'categories', 'abstract']\n",
    "\n",
    "articles_count = 10000\n",
    "\n",
    "elastic_data = {\n",
    "    'indices': {\n",
    "        'articles': {\n",
    "            'settings': {\n",
    "                'index.number_of_replicas': 0,\n",
    "                'index.number_of_shards': 1,\n",
    "            },\n",
    "            'mappings': {\n",
    "                'properties': {\n",
    "                    'id': {\"type\": \"keyword\"},\n",
    "                    'authors': {'type': 'keyword'},\n",
    "                    'last_modified': {'type': 'date'},\n",
    "                    'title': {'type': 'text'},\n",
    "                    'original_title': {'type': 'text'},\n",
    "                    'doi': {'type': 'keyword'},\n",
    "                    'categories': {'type': 'text'},\n",
    "                    'abstract': {'type': 'text'},\n",
    "                    'original_abstract': {'type': 'text'},\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "category_map = {\n",
    "    'astro-ph': 'Astrophysics',\n",
    "    'astro-ph.CO': 'Cosmology and Nongalactic Astrophysics',\n",
    "    'astro-ph.EP': 'Earth and Planetary Astrophysics',\n",
    "    'astro-ph.GA': 'Astrophysics of Galaxies',\n",
    "    'astro-ph.HE': 'High Energy Astrophysical Phenomena',\n",
    "    'astro-ph.IM': 'Instrumentation and Methods for Astrophysics',\n",
    "    'astro-ph.SR': 'Solar and Stellar Astrophysics',\n",
    "    'cond-mat.dis-nn': 'Disordered Systems and Neural Networks',\n",
    "    'cond-mat.mes-hall': 'Mesoscale and Nanoscale Physics',\n",
    "    'cond-mat.mtrl-sci': 'Materials Science',\n",
    "    'cond-mat.other': 'Other Condensed Matter',\n",
    "    'cond-mat.quant-gas': 'Quantum Gases',\n",
    "    'cond-mat.soft': 'Soft Condensed Matter',\n",
    "    'cond-mat.stat-mech': 'Statistical Mechanics',\n",
    "    'cond-mat.str-el': 'Strongly Correlated Electrons',\n",
    "    'cond-mat.supr-con': 'Superconductivity',\n",
    "    'cs.AI': 'Artificial Intelligence',\n",
    "    'cs.AR': 'Hardware Architecture',\n",
    "    'cs.CC': 'Computational Complexity',\n",
    "    'cs.CE': 'Computational Engineering, Finance, and Science',\n",
    "    'cs.CG': 'Computational Geometry',\n",
    "    'cs.CL': 'Computation and Language',\n",
    "    'cs.CR': 'Cryptography and Security',\n",
    "    'cs.CV': 'Computer Vision and Pattern Recognition',\n",
    "    'cs.CY': 'Computers and Society',\n",
    "    'cs.DB': 'Databases',\n",
    "    'cs.DC': 'Distributed, Parallel, and Cluster Computing',\n",
    "    'cs.DL': 'Digital Libraries',\n",
    "    'cs.DM': 'Discrete Mathematics',\n",
    "    'cs.DS': 'Data Structures and Algorithms',\n",
    "    'cs.ET': 'Emerging Technologies',\n",
    "    'cs.FL': 'Formal Languages and Automata Theory',\n",
    "    'cs.GL': 'General Literature',\n",
    "    'cs.GR': 'Graphics',\n",
    "    'cs.GT': 'Computer Science and Game Theory',\n",
    "    'cs.HC': 'Human-Computer Interaction',\n",
    "    'cs.IR': 'Information Retrieval',\n",
    "    'cs.IT': 'Information Theory',\n",
    "    'cs.LG': 'Machine Learning',\n",
    "    'cs.LO': 'Logic in Computer Science',\n",
    "    'cs.MA': 'Multiagent Systems',\n",
    "    'cs.MM': 'Multimedia',\n",
    "    'cs.MS': 'Mathematical Software',\n",
    "    'cs.NA': 'Numerical Analysis',\n",
    "    'cs.NE': 'Neural and Evolutionary Computing',\n",
    "    'cs.NI': 'Networking and Internet Architecture',\n",
    "    'cs.OH': 'Other Computer Science',\n",
    "    'cs.OS': 'Operating Systems',\n",
    "    'cs.PF': 'Performance',\n",
    "    'cs.PL': 'Programming Languages',\n",
    "    'cs.RO': 'Robotics',\n",
    "    'cs.SC': 'Symbolic Computation',\n",
    "    'cs.SD': 'Sound',\n",
    "    'cs.SE': 'Software Engineering',\n",
    "    'cs.SI': 'Social and Information Networks',\n",
    "    'cs.SY': 'Systems and Control',\n",
    "    'econ.EM': 'Econometrics',\n",
    "    'eess.AS': 'Audio and Speech Processing',\n",
    "    'eess.IV': 'Image and Video Processing',\n",
    "    'eess.SP': 'Signal Processing',\n",
    "    'gr-qc': 'General Relativity and Quantum Cosmology',\n",
    "    'hep-ex': 'High Energy Physics - Experiment',\n",
    "    'hep-lat': 'High Energy Physics - Lattice',\n",
    "    'hep-ph': 'High Energy Physics - Phenomenology',\n",
    "    'hep-th': 'High Energy Physics - Theory',\n",
    "    'math.AC': 'Commutative Algebra',\n",
    "    'math.AG': 'Algebraic Geometry',\n",
    "    'math.AP': 'Analysis of PDEs',\n",
    "    'math.AT': 'Algebraic Topology',\n",
    "    'math.CA': 'Classical Analysis and ODEs',\n",
    "    'math.CO': 'Combinatorics',\n",
    "    'math.CT': 'Category Theory',\n",
    "    'math.CV': 'Complex Variables',\n",
    "    'math.DG': 'Differential Geometry',\n",
    "    'math.DS': 'Dynamical Systems',\n",
    "    'math.FA': 'Functional Analysis',\n",
    "    'math.GM': 'General Mathematics',\n",
    "    'math.GN': 'General Topology',\n",
    "    'math.GR': 'Group Theory',\n",
    "    'math.GT': 'Geometric Topology',\n",
    "    'math.HO': 'History and Overview',\n",
    "    'math.IT': 'Information Theory',\n",
    "    'math.KT': 'K-Theory and Homology',\n",
    "    'math.LO': 'Logic',\n",
    "    'math.MG': 'Metric Geometry',\n",
    "    'math.MP': 'Mathematical Physics',\n",
    "    'math.NA': 'Numerical Analysis',\n",
    "    'math.NT': 'Number Theory',\n",
    "    'math.OA': 'Operator Algebras',\n",
    "    'math.OC': 'Optimization and Control',\n",
    "    'math.PR': 'Probability',\n",
    "    'math.QA': 'Quantum Algebra',\n",
    "    'math.RA': 'Rings and Algebras',\n",
    "    'math.RT': 'Representation Theory',\n",
    "    'math.SG': 'Symplectic Geometry',\n",
    "    'math.SP': 'Spectral Theory',\n",
    "    'math.ST': 'Statistics Theory',\n",
    "    'math-ph': 'Mathematical Physics',\n",
    "    'nlin.AO': 'Adaptation and Self-Organizing Systems',\n",
    "    'nlin.CD': 'Chaotic Dynamics',\n",
    "    'nlin.CG': 'Cellular Automata and Lattice Gases',\n",
    "    'nlin.PS': 'Pattern Formation and Solitons',\n",
    "    'nlin.SI': 'Exactly Solvable and Integrable Systems',\n",
    "    'nucl-ex': 'Nuclear Experiment',\n",
    "    'nucl-th': 'Nuclear Theory',\n",
    "    'physics.acc-ph': 'Accelerator Physics',\n",
    "    'physics.ao-ph': 'Atmospheric and Oceanic Physics',\n",
    "    'physics.app-ph': 'Applied Physics',\n",
    "    'physics.atm-clus': 'Atomic and Molecular Clusters',\n",
    "    'physics.atom-ph': 'Atomic Physics',\n",
    "    'physics.bio-ph': 'Biological Physics',\n",
    "    'physics.chem-ph': 'Chemical Physics',\n",
    "    'physics.class-ph': 'Classical Physics',\n",
    "    'physics.comp-ph': 'Computational Physics',\n",
    "    'physics.data-an': 'Data Analysis, Statistics and Probability',\n",
    "    'physics.ed-ph': 'Physics Education',\n",
    "    'physics.flu-dyn': 'Fluid Dynamics',\n",
    "    'physics.gen-ph': 'General Physics',\n",
    "    'physics.geo-ph': 'Geophysics',\n",
    "    'physics.hist-ph': 'History and Philosophy of Physics',\n",
    "    'physics.ins-det': 'Instrumentation and Detectors',\n",
    "    'physics.med-ph': 'Medical Physics',\n",
    "    'physics.optics': 'Optics',\n",
    "    'physics.plasm-ph': 'Plasma Physics',\n",
    "    'physics.pop-ph': 'Popular Physics',\n",
    "    'physics.soc-ph': 'Physics and Society',\n",
    "    'physics.space-ph': 'Space Physics',\n",
    "    'q-bio.BM': 'Biomolecules',\n",
    "    'q-bio.CB': 'Cell Behavior',\n",
    "    'q-bio.GN': 'Genomics',\n",
    "    'q-bio.MN': 'Molecular Networks',\n",
    "    'q-bio.NC': 'Neurons and Cognition',\n",
    "    'q-bio.OT': 'Other Quantitative Biology',\n",
    "    'q-bio.PE': 'Populations and Evolution',\n",
    "    'q-bio.QM': 'Quantitative Methods',\n",
    "    'q-bio.SC': 'Subcellular Processes',\n",
    "    'q-bio.TO': 'Tissues and Organs',\n",
    "    'q-fin.CP': 'Computational Finance',\n",
    "    'q-fin.EC': 'Economics',\n",
    "    'q-fin.GN': 'General Finance',\n",
    "    'q-fin.MF': 'Mathematical Finance',\n",
    "    'q-fin.PM': 'Portfolio Management',\n",
    "    'q-fin.PR': 'Pricing of Securities',\n",
    "    'q-fin.RM': 'Risk Management',\n",
    "    'q-fin.ST': 'Statistical Finance',\n",
    "    'q-fin.TR': 'Trading and Market Microstructure',\n",
    "    'quant-ph': 'Quantum Physics',\n",
    "    'stat.AP': 'Applications',\n",
    "    'stat.CO': 'Computation',\n",
    "    'stat.ME': 'Methodology',\n",
    "    'stat.ML': 'Machine Learning',\n",
    "    'stat.OT': 'Other Statistics',\n",
    "    'stat.TH': 'Statistics Theory'\n",
    "}\n",
    "\n",
    "print_divider = '\\n*************************************************************************\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb056eb",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193e3fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pypdf\n",
    "from io import BytesIO\n",
    "import scipdf\n",
    "import time\n",
    "# import urllib\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\n",
    "import en_core_sci_lg  # import downlaoded model\n",
    "import string\n",
    "# Parser\n",
    "parser = en_core_sci_lg.load()\n",
    "parser.max_length = 7000000 #Limit the size of the parser\n",
    "punctuations = string.punctuation #list of punctuation to remove from text\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords = list(STOP_WORDS)\n",
    "\n",
    "def preprocess(sentence):\n",
    "    parsed_sentence = parser(sentence.replace('\\n', ' '))\n",
    "    tokens = [word.lemma_.lower().strip() \n",
    "                if word.lemma_ != \"-PRON-\" \n",
    "                else word.lower_ for word in parsed_sentence\n",
    "             ] # transform to lowercase and then split the scentence\n",
    "    return ' '.join([unidecode(word) for word in tokens \n",
    "                    if word not in stopwords and word not in punctuations\n",
    "                    ]) #remove stopsword an punctuation\n",
    "\n",
    "def convert_pdf_bytes_to_str(pdf_bytes):\n",
    "    reader = pypdf.PdfReader(BytesIO(pdf_bytes))\n",
    "    return ' '.join([page.extract_text() for page in reader.pages])\n",
    "\n",
    "def clean_id(dir: str):\n",
    "    return dir.replace('/', '_')\n",
    "\n",
    "def get_article_text(article_id: str, folder: str):\n",
    "    text = ''\n",
    "    local_path = f'{folder}/{clean_id(article_id)}'\n",
    "    if not os.path.isdir(folder):\n",
    "        os.makedirs(folder)\n",
    "    if os.path.isfile(local_path):\n",
    "        with open(local_path, 'r') as file:\n",
    "            text = '\\n'.join(file.readlines())\n",
    "    else:\n",
    "        article_url = f'{base_article_url}/{article_id}'\n",
    "        response = requests.get(article_url)\n",
    "        text = convert_pdf_bytes_to_str(response.content)\n",
    "        with open(local_path, 'w') as file:\n",
    "            file.write(text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee5b87fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "articles index updated\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from elasticsearch import Elasticsearch as ElasticsearchClient\n",
    "from elasticsearch import NotFoundError\n",
    "\n",
    "\n",
    "class EsConfig:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.host: str = kwargs[\"host\"]\n",
    "        self.port: int = kwargs[\"port\"]\n",
    "        self.request_timeout: str = kwargs[\"request_timeout\"]\n",
    "        self.max_retries: int = kwargs[\"max_retries\"]\n",
    "\n",
    "\n",
    "class Elasticsearch:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.client = ElasticsearchClient(\n",
    "                    hosts=\"http://\" + self.config.host + \":\" + self.config.port,\n",
    "                    request_timeout=int(self.config.request_timeout[:-1]),\n",
    "                    retry_on_timeout=True,\n",
    "                    max_retries=self.config.max_retries,\n",
    "                )\n",
    "        self._create_or_update_indices()\n",
    "\n",
    "    def _create_or_update_indices(self):\n",
    "        for index_name in elastic_data['indices']:\n",
    "            existing_indices = self._cat_indices(index_name)\n",
    "            if not existing_indices or len(existing_indices) == 0:\n",
    "                self._create_index(index_name)\n",
    "                print(f\"created articles index\")\n",
    "            else:\n",
    "                self._update_index(index_name)\n",
    "                print(f\"articles index updated\")\n",
    "\n",
    "\n",
    "    def _cat_indices(self, name: str) -> Dict:\n",
    "        response = []\n",
    "        try:\n",
    "            response = self.client.cat.indices(\n",
    "                index=name,\n",
    "                s=\"creation.date\",\n",
    "                h=\"index,docs.count,creation.date\",\n",
    "                format=\"json\",\n",
    "            )\n",
    "        except NotFoundError:\n",
    "            return []\n",
    "        return response\n",
    "\n",
    "\n",
    "    def _create_index(self, index_name):\n",
    "        settings = elastic_data['indices'][index_name]['settings']\n",
    "        mappings = elastic_data['indices'][index_name]['mappings']\n",
    "        self.client.indices.create(index=index_name, settings=settings, mappings=mappings)\n",
    "\n",
    "    def _update_index(self, index_name):\n",
    "        mapping_properties = elastic_data['indices'][index_name]['mappings']['properties']\n",
    "        self.client.indices.put_mapping(index=index_name, properties=mapping_properties)\n",
    "\n",
    "\n",
    "    def index(self, index_name, document, id_):\n",
    "        return self.client.index(\n",
    "            index=index_name,\n",
    "            document=document,\n",
    "            id=id_,\n",
    "            timeout=self.config.request_timeout,\n",
    "        )\n",
    "\n",
    "    def search(self, queries, n):\n",
    "        query = {\n",
    "                \"bool\": {\n",
    "                    \"should\": [\n",
    "                        {\"match\": {field: value}} for field, value in queries if value is not None and value != \"\" \n",
    "                    ]\n",
    "                },\n",
    "            }\n",
    "        return self.client.search(\n",
    "            query=query,\n",
    "            size=n,\n",
    "        )\n",
    "\n",
    "    def get_article(self, article_id: str) -> Dict:\n",
    "        return es.client.get(\n",
    "                index='articles',\n",
    "                id=article_id,\n",
    "            )['_source']\n",
    "\n",
    "\n",
    "es_config = EsConfig(host='localhost', port='9200', request_timeout= '3s', max_retries=10)\n",
    "es = Elasticsearch(config=es_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f23e10ff",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619ffda",
   "metadata": {},
   "source": [
    "# Index Documents into Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "609889d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def populate_elasticsearch():\n",
    "    with open(arxiv_complete_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            article = json.loads(line)\n",
    "            doc = {\n",
    "                'id': clean_id(article['id']),\n",
    "                'authors': [' '.join(author_array) for author_array in article['authors_parsed']],\n",
    "                'last_modified': article['update_date'],\n",
    "                'title': preprocess(article['title']),\n",
    "                'original_title': article['title'],\n",
    "                'doi': article['doi'],\n",
    "                'categories': article['categories'].split(' '),\n",
    "                'abstract': preprocess(article['abstract']),\n",
    "                'original_abstract': article['abstract'],\n",
    "            }\n",
    "            es.index(\n",
    "                index_name='articles',\n",
    "                document=doc,\n",
    "                id_=doc['id'],\n",
    "            )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7499c599",
   "metadata": {},
   "source": [
    "Obfuscation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b18fa0b0",
   "metadata": {},
   "source": [
    "    0. Download 1000 random articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d78363b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import random\n",
    "\n",
    "if False:\n",
    "    for i in range(1):\n",
    "        doc_number = random.randint(articles_count)\n",
    "        doc_id = es.client.search(\n",
    "            index='articles',\n",
    "            query={'match_all':{}},\n",
    "            from_=doc_number,\n",
    "            size=1,\n",
    "        )['hits']['hits'][0]['_source']['id']\n",
    "        try:\n",
    "            doc_text = get_article_text(doc_id, articles_to_plagiarise_dir)\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "90b66f1a",
   "metadata": {},
   "source": [
    "Create Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "632085cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "if True:\n",
    "\n",
    "    metadata = {\n",
    "        'exact_copies_ids': [],\n",
    "        'near_copies_ids': [],\n",
    "        'substitutes_ids': [],\n",
    "        'paraphrases_ids': [],\n",
    "    }\n",
    "    for i, article_id in enumerate(os.listdir(articles_to_plagiarise_dir)):\n",
    "        if i < exact_copies_count:\n",
    "            metadata['exact_copies_ids'].append(article_id)\n",
    "        elif i - exact_copies_count < near_copies_count:\n",
    "            metadata['near_copies_ids'].append(article_id)\n",
    "        elif i - exact_copies_count - near_copies_count < substitutes_count:\n",
    "            metadata['substitutes_ids'].append(article_id)\n",
    "        else:\n",
    "            metadata['paraphrases_ids'].append(article_id)\n",
    "\n",
    "    test_metadata_dir = ''.join(test_metadata_path.split('/')[:-1])\n",
    "    if not os.path.isdir(test_metadata_dir):\n",
    "        os.makedirs(test_metadata_dir)\n",
    "\n",
    "    with open(test_metadata_path, 'w') as file:\n",
    "        json.dump(metadata, file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f5245e8",
   "metadata": {},
   "source": [
    "Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb61ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "metadata = None\n",
    "with open(test_metadata_path, 'r') as file:\n",
    "    metadata = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc6fdf63",
   "metadata": {},
   "source": [
    "Load Base Test Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f42122e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base title: Non-Abelian Hopf Cohomology II -- the General Case --\n",
      "*************************************************************************\n",
      "base abstract:   We introduce and study non-abelian cohomology sets of Hopf algebras with\n",
      "coefficients in Hopf comodule algebras. We prove that these sets generalize as\n",
      "well Serre's non-abelian group cohomology theory as the cohomological theory\n",
      "constructed by the authors in a previous article. We establish their\n",
      "functoriality and compute explicit examples. Further we classify Hopf torsors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "base_test_article = {\"id\":\"0706.0638\",\"submitter\":\"Marc Wambst\",\"authors\":\"Philippe Nuss (IRMA), Marc Wambst (IRMA)\",\"title\":\"Non-Abelian Hopf Cohomology II -- the General Case --\",\"comments\":\"This paper is conceived as the continuation of the article of the\\n  same authors entitled \\\"Non-Abelian Hopf Cohomology\\\", J. Algebra t. 312,\\n  (2007), no 2, p. 733 -- 754\",\"journal-ref\":\"Journal of Algebra 319 (2008) 4621--4645\",\"doi\":None,\"report-no\":None,\"categories\":\"math.QA\",\"license\":None,\"abstract\":\"  We introduce and study non-abelian cohomology sets of Hopf algebras with\\ncoefficients in Hopf comodule algebras. We prove that these sets generalize as\\nwell Serre's non-abelian group cohomology theory as the cohomological theory\\nconstructed by the authors in a previous article. We establish their\\nfunctoriality and compute explicit examples. Further we classify Hopf torsors.\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Tue, 5 Jun 2007 12:19:11 GMT\"}],\"update_date\":\"2008-05-15\",\"authors_parsed\":[[\"Nuss\",\"Philippe\",\"\",\"IRMA\"],[\"Wambst\",\"Marc\",\"\",\"IRMA\"]]}\n",
    "base_test_article = {\n",
    "    'id': clean_id(base_test_article['id']),\n",
    "    'authors': [' '.join(author_array) for author_array in base_test_article['authors_parsed']],\n",
    "    'last_modified': base_test_article['update_date'],\n",
    "    'title': preprocess(base_test_article['title']),\n",
    "    'original_title': base_test_article['title'],\n",
    "    'doi': base_test_article['doi'],\n",
    "    'categories': base_test_article['categories'].split(' '),\n",
    "    'abstract': preprocess(base_test_article['abstract']),\n",
    "    'original_abstract': base_test_article['abstract'],\n",
    "}\n",
    "base_test_article_text = get_article_text(base_test_article_id, articles_to_plagiarise_dir)\n",
    "base_title_tokenized = sent_tokenize(base_test_article['original_title'])\n",
    "print(f'base title: {base_test_article[\"original_title\"]}', end=print_divider)\n",
    "print(f'base abstract: {base_test_article[\"original_abstract\"]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0d0b8d",
   "metadata": {},
   "source": [
    "    1. Exact Copy + rearrangement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9370c6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from numpy import random\n",
    "import re\n",
    "\n",
    "def simple_exact_text_copier(original_text: str, base_text: str, max_copy_count: int = None, max_copy_percentage: int = None):\n",
    "    tmp = sent_tokenize(re.sub('[\\n ]+', ' ', original_text))\n",
    "    org_text_tokenized = [sentence for sentence in tmp if len(word_tokenize(sentence.strip())) > 10] or [tmp[0]]\n",
    "    new_text_tokenized = sent_tokenize(re.sub('[\\n ]+', ' ', base_text))\n",
    "    if max_copy_percentage:\n",
    "       max_copy_count = int(len(org_text_tokenized) * (max_copy_percentage / 100))\n",
    "    sample_size = max(1, min(max_copy_count, len(org_text_tokenized)))\n",
    "    indexes_to_insert = random.choice(len(new_text_tokenized), size=sample_size)\n",
    "\n",
    "    org_text_indexes = random.choice(len(org_text_tokenized), size=sample_size)\n",
    "    for i in range(sample_size):\n",
    "        sentence_to_copy = org_text_tokenized[org_text_indexes[i]]\n",
    "        idx_to_insert = indexes_to_insert[i]\n",
    "        new_text_tokenized.insert(idx_to_insert, sentence_to_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f668cb",
   "metadata": {},
   "source": [
    "    2. Near Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95d1ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def delete_random_words(original_text: str, deletion_percentage: int):\n",
    "    org_text_tokenized = word_tokenize(original_text)\n",
    "    deletion_count = max(int(len(org_text_tokenized) * (deletion_percentage / 100)), 1)\n",
    "    indexes_to_delete = numpy.random.choice(len(org_text_tokenized), size=deletion_count)\n",
    "    new_text = []\n",
    "    for i in range(len(org_text_tokenized)):\n",
    "        if i not in indexes_to_delete:\n",
    "            new_text.append(org_text_tokenized[i])\n",
    "    \n",
    "    return ' '.join(new_text)\n",
    "\n",
    "\n",
    "def make_near_copy(article_id: str):\n",
    "    original_article = es.get_article(article_id)\n",
    "    original_text = get_article_text(article_id, articles_to_plagiarise_dir)\n",
    "    pruned_title = delete_random_words(original_article['original_title'], 20)\n",
    "    pruned_abstract = delete_random_words(original_article['original_abstract'], 30)\n",
    "    pruned_text = delete_random_words(original_text, 40)\n",
    "    exact_plagiarised_article = make_exact_copy(article_id)\n",
    "    plagiarised_article = {\n",
    "        'id': original_article['id'],\n",
    "        'title': simple_exact_text_copier(pruned_title, base_test_article['original_title'], max_copy_percentage=60),\n",
    "        'abstract': simple_exact_text_copier(pruned_abstract, base_test_article['original_abstract'], max_copy_percentage=40),\n",
    "        'text': simple_exact_text_copier(pruned_text, base_test_article_text, max_copy_percentage=10),\n",
    "        'plagiarism_type': 'near_copy',\n",
    "    }\n",
    "    return plagiarised_article\n",
    "\n",
    "if True:\n",
    "    import json\n",
    "    if not os.path.isdir(test_articles_dir):\n",
    "        os.makedirs(test_articles_dir)\n",
    "    near_copies_path = f'{test_articles_dir}/near_copies.json'\n",
    "    with open(near_copies_path, 'w') as file:\n",
    "        for i in range(near_copies_count):\n",
    "            original_article_id = metadata['near_copies_ids'][i]\n",
    "            plagiarised_article = make_near_copy(original_article_id)\n",
    "            file.write(json.dumps(plagiarised_article) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7712ae7d",
   "metadata": {},
   "source": [
    "    3. Synonym Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "727e85a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def synonym(word: str):\n",
    "    try: \n",
    "        print(f'syn({word}) = {wordnet.synsets(word)[0].lemmas()[0].name()}')\n",
    "        wordnet.synset(word).w\n",
    "        return wordnet.synsets(word)[0].lemmas()[0].name()\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def substitute_random_words(original_text: str, substitution_percentage: int):\n",
    "    org_text_tokenized = word_tokenize(original_text)\n",
    "    substitution_count = max(1, int(len(org_text_tokenized) * (substitution_percentage / 100)))\n",
    "    indexes_to_substitute = numpy.random.choice(len(org_text_tokenized), size=substitution_percentage)\n",
    "    sbustituted_text_tokenized = org_text_tokenized\n",
    "    for ind in indexes_to_substitute:\n",
    "        sbustituted_text_tokenized[ind] = synonym(org_text_tokenized[ind]) \n",
    "    \n",
    "    return ' '.join(sbustituted_text_tokenized)\n",
    "\n",
    "def near_copy(article_id: str):\n",
    "    original_article = es.get_article(article_id)\n",
    "    original_text = get_article_text(article_id, articles_to_plagiarise_dir)\n",
    "    substituted_title = substitute_random_words(orgiinal_article['original_title'], 20)\n",
    "    substituted_abstract = substitute_random_words(orgiinal_article['original_abstract'], 20)\n",
    "    substituted_text = substitute_random_words(original_text, 20)\n",
    "    plagiarised_article = {\n",
    "        'id': original_article['id'],\n",
    "        'title': simple_exact_text_copier(substituted_title, base_test_article['original_title'], max_copy_percentage=60),\n",
    "        'abstract': simple_exact_text_copier(substituted_abstract, base_test_article['original_abstract'], max_copy_percentage=40),\n",
    "        'text': simple_exact_text_copier(substituted_text, base_test_article_text, max_copy_percentage=40),\n",
    "        'plagiarism_type': 'near_copy',\n",
    "    }\n",
    "    return plagiarised_article\n",
    "\n",
    "if True:\n",
    "    import json\n",
    "    if not os.path.isdir(test_articles_dir):\n",
    "        os.makedirs(test_articles_dir)\n",
    "    substitutes_path = f'{test_articles_dir}/substitutes.json'\n",
    "    with open(substitutes_path, 'w') as file:\n",
    "        for i in range(substitutes_count):\n",
    "            original_article_id = metadata['substitutes_ids'][i]\n",
    "            plagiarised_article = make_near_copy(original_article_id)\n",
    "            file.write(json.dumps(plagiarised_article) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9026031e",
   "metadata": {},
   "source": [
    "    4. Plagiarism by paraphrasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ea169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if False:\n",
    "    import json\n",
    "    if not os.path.isdir('articles_to_paraphrase/'):\n",
    "        os.makedirs('articles_to_paraphrase/')\n",
    "    paraphrases_path = f'articles_to_paraphrase/paraphrases.json'\n",
    "    with open(paraphrases_path, 'w') as file:\n",
    "        for i in range(paraphrases_count):\n",
    "            original_article_id = metadata['paraphrases_ids'][i]\n",
    "            original_article = es.get_article(original_article_id)\n",
    "            original_text = get_article_text(original_article_id, articles_to_plagiarise_dir)\n",
    "            plagiarised_article = {\n",
    "                'id': original_article['id'],\n",
    "                'title': original_article['title'],\n",
    "                'abstract': original_article['original_abstract'],\n",
    "                'text': original_text,\n",
    "                'plagiarism_type': 'paraphrasing'\n",
    "            }\n",
    "            file.write(json.dumps(plagiarised_article) + '\\n')\n",
    "    \n",
    "    # the rest is done by hand + online paraphrasing tools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dff36bd",
   "metadata": {},
   "source": [
    "    Not Plagiarised Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b49bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if True:\n",
    "    import json\n",
    "    not_plagiarised_path = f'test_articles/not-plagiarised.json'\n",
    "    incomplete_articles_path = f'not-plagiarised-articles/not-plagiarised.json'\n",
    "    with open(not_plagiarised_path, 'w') as file_to_write:\n",
    "        with open(incomplete_articles_path, 'r') as file_to_read:\n",
    "            for line in file_to_read.readlines():\n",
    "                article = json.loads(line)\n",
    "                original_text = get_article_text(article['id'], articles_to_plagiarise_dir)\n",
    "                final_article = {\n",
    "                    'id': article['id'],\n",
    "                    'title': article['title'],\n",
    "                    'abstract': article['abstract'],\n",
    "                    'text': original_text,\n",
    "                    'plagiarism_type': 'not-plagiarised'\n",
    "                }\n",
    "                file_to_write.write(json.dumps(final_article) + '\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab0e97bf",
   "metadata": {},
   "source": [
    "    Declare Detection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e25b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sentence_transformers import util\n",
    "import nltk\n",
    "import numpy\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def find_similarities(sentences_a: str, sentences_b: str):\n",
    "    a_tokenized = [preprocess(sentence.replace('\\n', ' ').strip()) for sentence in sentences_a \n",
    "                    if len(word_tokenize(sentence.strip())) > 10] \\\n",
    "                    or [preprocess(sentences_a[0].replace('\\n', ' ').strip())]\n",
    "    b_tokenized = [preprocess(sentence.replace('\\n', ' ').strip()) for sentence in sentences_b \n",
    "                    if len(word_tokenize(sentence.strip())) > 10] \\\n",
    "                    or [preprocess(sentences_b[0].replace('\\n', ' ').strip())]\n",
    "\n",
    "    a_encoded = numpy.array([model.encode(sentence) for sentence in a_tokenized])\n",
    "    b_encoded = numpy.array([model.encode(sentence) for sentence in b_tokenized])\n",
    "\n",
    "    return a_tokenized, b_tokenized, util.pytorch_cos_sim(a_encoded, b_encoded)\n",
    "\n",
    "def compare_sentences(sentences_a: str, sentences_b: str):\n",
    "    sentences_a, sentences_b, res = find_similarities(sentences_a, sentences_b)\n",
    "    flat = res.flatten()\n",
    "    k = min(len(flat), 10)\n",
    "    topk = torch.topk(flat, k)\n",
    "    return topk[0][0] > 0.85\n",
    "\n",
    "\n",
    "def abstract_query_extractor(article: Dict):\n",
    "    return [('abstract', article['abstract'])]\n",
    "\n",
    "def whole_article_query_extractor(article: Dict):\n",
    "    indexes = numpy.random.choice(len(article['text_tokenized']), size=100)\n",
    "    queries = [('text', article['text_tokenized'][index]) for index in indexes] + [\n",
    "        ('title', article['title']),\n",
    "        ('abstract', article['abstract']),\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "def abstract_comparator(article_1: Dict, article_2: Dict):\n",
    "    return [article_1['abstract']], [article_2['abstract']]\n",
    "\n",
    "def whole_article_comparator(article_1: Dict, article_2: Dict):\n",
    "    return [article_1['title'], article_1['abstract']] + article_1['text_tokenized'], \\\n",
    "        [article_2['title'], article_2['abstract']] + article_2['text_tokenized']\n",
    "\n",
    "query_extraction_methods = {\n",
    "    'abstract': abstract_query_extractor,\n",
    "    'whole': whole_article_query_extractor,\n",
    "}\n",
    "\n",
    "article_comparators = {\n",
    "    'abstract': abstract_comparator,\n",
    "    'whole': whole_article_comparator,\n",
    "}\n",
    "\n",
    "plagiarism_detection_methods = {\n",
    "    f'q:{query_extraction_method}_c:{article_comparison_method}': {\n",
    "        'query_extractor': query_extractor,\n",
    "        'article_comparator': article_comparator,\n",
    "    } for query_extraction_method, query_extractor in query_extraction_methods.items()\n",
    "        for article_comparison_method, article_comparator in article_comparators.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1796abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy and recall of abstract_abstract on paraphrases are 0.5 and 0.5\n",
      "accuracy and recall of abstract_whole on paraphrases are 1.0 and 1.0\n",
      "accuracy and recall of whole_abstract on paraphrases are 0.5 and 0.5\n",
      "accuracy and recall of whole_whole on paraphrases are 1.0 and 1.0\n",
      "accuracy and recall of abstract_abstract on exact_copies are 0.0 and 0.0\n",
      "accuracy and recall of abstract_whole on exact_copies are 0.75 and 0.75\n",
      "accuracy and recall of whole_abstract on exact_copies are 0.0 and 0.0\n",
      "accuracy and recall of whole_whole on exact_copies are 1.0 and 1.0\n",
      "accuracy and recall of abstract_abstract on substitutes are 0.0 and 0.0\n",
      "accuracy and recall of abstract_whole on substitutes are 0.75 and 0.75\n",
      "accuracy and recall of whole_abstract on substitutes are 0.0 and 0.0\n",
      "accuracy and recall of whole_whole on substitutes are 0.75 and 0.75\n",
      "accuracy and recall of abstract_abstract on not-plagiarised are 1.0 and 1\n",
      "accuracy and recall of abstract_whole on not-plagiarised are 1.0 and 1\n",
      "accuracy and recall of whole_abstract on not-plagiarised are 1.0 and 1\n",
      "accuracy and recall of whole_whole on not-plagiarised are 1.0 and 1\n",
      "accuracy and recall of abstract_abstract on near_copies are 0.0 and 0.0\n",
      "accuracy and recall of abstract_whole on near_copies are 0.5 and 0.5\n",
      "accuracy and recall of whole_abstract on near_copies are 0.0 and 0.0\n",
      "accuracy and recall of whole_whole on near_copies are 0.5 and 0.5\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "\n",
    "correct_guesses = {\n",
    "    'abstract_abstract': {True: 0, False: 0},\n",
    "    'abstract_whole': {True: 0, False: 0},\n",
    "    'whole_abstract': {True: 0, False: 0},\n",
    "    'whole_whole': {True: 0, False: 0},\n",
    "}\n",
    "total = {True: 0, False: 0}\n",
    "\n",
    "\n",
    "def compute_accuracy(plagiarism_detetion_method: str):\n",
    "    return (correct_guesses[plagiarism_detection_method][True] + correct_guesses[plagiarism_detection_method][False]) / (total[True] + total[False])\n",
    "\n",
    "def compute_recall(plagiarism_detection_method: str):\n",
    "    if total[True]:\n",
    "        return correct_guesses[plagiarism_detection_method][True] / total[True]\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "with open('information-retrieval-results.csv', 'w') as ir_results_file:\n",
    "    ir_csv_writer = csv.writer(ir_results_file)\n",
    "    ir_csv_writer.writerow([\"id\",\"plagiarism_type\",\"abstract_abstract\",\"abstract_whole\",\"whole_abstract\",\"whole_whole\"])\n",
    "\n",
    "    with open('similarity-results.csv', 'w') as similarity_results_file:\n",
    "        sim_csv_writer = csv.writer(similarity_results_file)\n",
    "        sim_csv_writer.writerow([\"id\",\"plagiarism_type\",\"abstract_abstract\",\"abstract_whole\",\"whole_abstract\",\"whole_whole\"])\n",
    "        for file_name in os.listdir(test_articles_dir):\n",
    "            # if file_name != 'not-plagiarised.json':\n",
    "            # if file_name in ['paraphrases.json', 'not-plagiarised.json']:\n",
    "                # continue\n",
    "            with open(f'{test_articles_dir}/{file_name}', 'r') as file:\n",
    "                correct_guesses = {\n",
    "                    'abstract_abstract': {True: 0, False: 0},\n",
    "                    'abstract_whole': {True: 0, False: 0},\n",
    "                    'whole_abstract': {True: 0, False: 0},\n",
    "                    'whole_whole': {True: 0, False: 0},\n",
    "                }\n",
    "                total = {True: 0, False: 0}\n",
    "\n",
    "                for i, line in enumerate(file.readlines()):\n",
    "                    if i == 4:\n",
    "                        break\n",
    "                    test_article = json.loads(line)\n",
    "                    \n",
    "                    test_article['text_tokenized'] = sent_tokenize(re.sub('[ \\n]+', ' ', test_article['text']))\n",
    "\n",
    "                    # print('the first few sentences of test article are: ')\n",
    "                    # for i in range(min(len(test_article['text_tokenized']), 4)):\n",
    "                    #     print(test_article['text_tokenized'][i], end='\\n-------------\\n')\n",
    "\n",
    "                    # print(print_divider)\n",
    "\n",
    "\n",
    "                    sim_result_row = {'id': test_article['id'], 'plagiarism_type': test_article['plagiarism_type']}\n",
    "                    ir_result_row = {'id': test_article['id'], 'plagiarism_type': test_article['plagiarism_type']}\n",
    "                    is_article_plagiarised = test_article['plagiarism_type'] != 'not-plagiarised'\n",
    "                    total[is_article_plagiarised] += 1\n",
    "\n",
    "                    for query_extraction_method, query_extractor in query_extraction_methods.items():\n",
    "                        for article_comparison_method, article_comparator in article_comparators.items(): \n",
    "                            plagiarism_detection_method = query_extraction_method + '_' + article_comparison_method\n",
    "                            # if plagiarism_detection_method != 'whole_whole':\n",
    "                                # continue\n",
    "\n",
    "                            queries = query_extractor(test_article)\n",
    "                            elasticsearch_results = es.search(queries=queries, n=1)\n",
    "\n",
    "                            for i, es_res in enumerate(elasticsearch_results['hits']['hits']):\n",
    "                                candidate_article = es_res['_source']\n",
    "\n",
    "                                candidate_article_text_raw = get_article_text(candidate_article['id'], articles_to_plagiarise_dir)\n",
    "                                candidate_article_text = re.sub('[ \\n]+', ' ', candidate_article_text_raw)\n",
    "\n",
    "                                candidate_article.update({\n",
    "                                    'text': candidate_article_text,\n",
    "                                    'text_tokenized': sent_tokenize(candidate_article_text),\n",
    "                                })\n",
    "                                test_article_sentences, candidate_article_sentencess = article_comparator(test_article, candidate_article)\n",
    "                                plagiarism_detected = bool(compare_sentences(test_article_sentences, candidate_article_sentencess))\n",
    "                                sim_result_row[plagiarism_detection_method] = plagiarism_detected\n",
    "                                ir_result_row[plagiarism_detection_method] = candidate_article['id'] == test_article['id']\n",
    "                                if plagiarism_detected == is_article_plagiarised:\n",
    "                                    correct_guesses[plagiarism_detection_method][plagiarism_detected] += 1\n",
    "                    sim_csv_writer.writerow(sim_result_row.values())\n",
    "                    ir_csv_writer.writerow(ir_result_row.values())\n",
    "            for plagiarism_detection_method in correct_guesses:\n",
    "                print(f'accuracy and recall of {plagiarism_detection_method} on {file_name.split(\".\")[0]} are {compute_accuracy(plagiarism_detection_method)} and {compute_recall(plagiarism_detection_method)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70322485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(len(sent_tokenize(s1))) # 315\n",
    "# print(res.shape) torch.Size([315, 231])\n",
    "\n",
    "    # break\n",
    "    # print('*' * 20 + f'\\nthe following sentences have a similarity of {topk[0][i]}:')\n",
    "    # a_index, b_index = topk[1][i] // len(sentences_a), topk[1][i] % len(sentences_a)\n",
    "    # a_sentence, b_sentence = sentences_a[a_index].replace('\\n', ' '), sentences_b[b_index].replace('\\n', ' ')\n",
    "    # print(f\"{a_sentence}\\n**************\\n{b_sentence}\\n\")\n",
    "# max_similarity_per_sentence = torch.amax(res, axis=1)\n",
    "# sorted_similarities = torch.sort(max_similarity_per_sentence)\n",
    "# print(sorted_similarities)\n",
    "# print(sorted_similarities[0][-10:], '\\n', sorted_similarities[1][-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de634700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dask_dataframe\n",
    "df = dask_dataframe.read_json(arxiv_complete_path, blocksize=25e6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84077809",
   "metadata": {},
   "source": [
    "Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "914174c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.utils import io\n",
    "with io.capture_output() as captured:\n",
    "    ! pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d7a8c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_sci_lg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mspacy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlang\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39men\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstop_words\u001b[39;00m \u001b[39mimport\u001b[39;00m STOP_WORDS \u001b[39m#import commen list of stopword\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39men_core_sci_lg\u001b[39;00m  \u001b[39m# import downlaoded model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstring\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Parser\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_sci_lg'"
     ]
    }
   ],
   "source": [
    "# df['processed_abstract'] = df['abstract'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b7b324",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plagiarism-detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "19e66bef26ce2317f7c16673f4f1566fbcf375bbeed95b1ef9c425a761ac3c64"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
